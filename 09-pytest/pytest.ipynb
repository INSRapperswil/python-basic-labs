{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with pytest\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will learn how to write automated tests for your code. Tests check that your code does what it's supposed to do, usually testing known good inputs/outputs of your functions, but also corner cases and error conditions. Here's a simple example:\n",
    "\n",
    "```python\n",
    "def add_two(n):\n",
    "    return n + 2\n",
    "\n",
    "def test_add_two():\n",
    "    assert add_two(2) == 4\n",
    "```\n",
    "\n",
    "The `assert` keyword makes sure that the test fails when the given condition is false. More details about that will follow below.\n",
    "\n",
    "Even if you're not writing huge and complex applications, it pays off to start writing tests early. Spending additional effort on tests might seem like an unneeded burden at first, especially for very simple projects! But with a bit of practice, it'll be straightforward to write tests in parallel to your  new code. Writing tests pays off sooner as you might think, because you don't need to execute your code manually all the time to verify it does what you intended.\n",
    "\n",
    "Tests allow you to introduce future changes to your code quickly, without having to be afraid of breaking things when changing your code. In addition, they provide additional hints about how your code works in different scenarios (or edge cases), as well as forcing you to look at your code from a different angle, often finding bugs in the process.\n",
    "\n",
    "Python comes with an integrated [unittest](https://docs.python.org/3/library/unittest.html) module, but writing tests using it is unnecessarily complex. Thus, we will instead be using the [pytest](https://docs.pytest.org/) testing framework. A core maintainer of pytest (Florian Bruhin) is working at the INS, but this is far from a personal choice - in fact, according to the [JetBrains Python Developers Survey 2020](https://www.jetbrains.com/lp/python-developers-survey-2020/) with more than 28'000 respondents, pytest is by far the most popular testing framework:\n",
    "\n",
    "![survey results](survey2020.png)\n",
    "\n",
    "Unfortunately, the \"Automate the Boring Stuff\" book does not cover testing at all. Thus, the \"Summary\" in here is a bit more in-depth than in previous labs.\n",
    "\n",
    "Additionally, here are some alternative resources:\n",
    "\n",
    "- [pytest Tech-Webinar at INS](https://bruhin.software/ins-pytest/) (video part of this lab)\n",
    "- [Effective Python Testing With Pytest – Real Python](https://realpython.com/pytest-python-testing/)\n",
    "- [pytest: helps you write better programs — pytest documentation](https://docs.pytest.org/en/stable/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Here be dragons!\n",
    "\n",
    "![Hic Sunt Dracones](https://upload.wikimedia.org/wikipedia/commons/c/cd/Lenox_Globe_Dragons.png)\n",
    "\n",
    "Some of the topics we will cover in this lab are quite advanced. Please don't let that discourage you, and feel free to ask questions! It's a tricky topic, because testing can touch upon various more advanced topics you haven't learned about in detail yet. Nevertheless it's an important topic to learn about as early as possible, as can be a very useful tool for later labs you solve.\n",
    "\n",
    "If this feels like too much, feel free to skip some of the later topics/exercises (e.g. about fixtures), but make sure you make yourself familiar with the basics of pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Before you can start using `pytest`, you will need to install it. To do so, please run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --user pytest ipytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This installs both pytest itself, as well as [ipytest](https://github.com/chmp/ipytest) for integration in the Jupyter Lab.\n",
    "\n",
    "Next, use the \"Kernel -> Restart\" menu to make sure `ipytest` is loaded, and finally run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig(addopts=[\"--color=yes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to verify that pytest is called correctly, first run it as an external process via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this works as expected, make sure it's possible to run pytest as part of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================ test session starts =============================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmpju7i_jp_.py \u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "def test_nothing():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about how this works below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "### Outside of the notebook\n",
    "\n",
    "In normal usage, pytest is an external command-line tool which reads Python code containing tests from a file.\n",
    "\n",
    "In some exercises, we will use this more \"regular\" mode of operation rather than the more convenient notebook integration - either due to how certain pytest/ipytest internals work, or because we want to run `pytest` over the same code multiple times without having to copy-paste the code.\n",
    "\n",
    "Typically, if your code resides in a `myapp.py`, you'd have a corresponding `test_myapp.py` file with your tests, and running `pytest` will discover them in the `test_*` file.\n",
    "\n",
    "To do the same thing from the notebook, first, we use a special `%%writefile` command as the first line of a cell to write its contents to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_via_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_via_file.py\n",
    "def test_in_a_file():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of command is called a \"cell magic\" by Jupyter, because it does something special with the contents of a cell.\n",
    "\n",
    "After running a cell with the `%%writefile` magic, you should see the file show up in the file tree on the left. Next, we can run `pytest` as an external command by using the special `!pytest` syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "[...]\n",
      "\n",
      "test_via_file.py \u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_via_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your day-to-day usage, instead of running `pytest` on the command-line, you can also use integrations in IDEs like VS Code or IntelliJ's PyCharm, which let you conveniently run individual tests right from the editor. It's still recommended to get yourself familiar with running pytest from the commandline, since various useful pytest features can only be used that way.\n",
    "\n",
    "### In the notebook\n",
    "\n",
    "With `ipytest` set up, we can use the special `%%run_pytest[clean]` cell magic, which saves the contents to a temporary file and then runs pytest over that file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmp058xzk4i.py \u001b[32m.\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================ \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "def test_math():\n",
    "    assert 1 + 1 == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you always use `%%run_pytest[clean]` rather than just `%%run_pytest`, to avoid collecting and running all previous tests you've written. It's currently not possible to set this as the default behavior, but an option to do so is currently [in discussion](https://github.com/chmp/ipytest/issues/57) with the `ipytest` maintainer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Autodiscovery\n",
    "\n",
    "When you run `pytest`, it will start in your current directory and **discover all files starting with `test_*.py`** (except when you pass filename(s) to it, as in `pytest test_things.py`). In the files it discovered, it will then find:\n",
    "\n",
    "- **All functions with a name starting with `test_`**\n",
    "- (All classes starting with `Test`)\n",
    "- (All classes inheriting from `unittest.TestCase` for compatibility with Python's built-in test runner)\n",
    "\n",
    "In this exercise, we will only cover plain test functions. In pytest, classes are only used to group related tests. You'll learn more about Python classes in lab 18.\n",
    "\n",
    "### Assertions\n",
    "\n",
    "To verify that a certain condition holds true, we use the `assert` statement built into Python. After the `assert`, there's a condition, the same way there would be after an `if` keyword. We can provoke a failing assertion even outside of pytest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "!python -c \"assert 1 + 1 == 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python will only tell us that an `AssertionError` happened, but without any details (we run `python` as an external command here to get the unaltered Python output - Jupyter Lab and `ipytest` change the output in some ways).\n",
    "\n",
    "This is where `pytest` comes in, which will interpret those assertions in a special way, so that it's able to give us more information. If we move the assertion into a test function and run `pytest` over it, we will see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmp6bzqpgfo.py \u001b[31mF\u001b[0m\u001b[31m                             [100%]\u001b[0m\n",
      "\n",
      "===================== FAILURES =====================\n",
      "\u001b[31m\u001b[1m_________________ test_wrong_math __________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_wrong_math\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94m1\u001b[39;49;00m + \u001b[94m1\u001b[39;49;00m == \u001b[94m3\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert (1 + 1) == 3\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-10-67533e027c88>\u001b[0m:2: AssertionError\n",
      "============= short test summary info ==============\n",
      "FAILED tmp6bzqpgfo.py::test_wrong_math - assert (...\n",
      "\u001b[31m================ \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "def test_wrong_math():\n",
    "    assert 1 + 1 == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it might be immediately obvious what's wrong, but for more complex situations (imagine values coming from a server or user input), this \"enriched\" output can be very useful.\n",
    "\n",
    "**Note:** There are no parentheses after `assert`, the syntax is `assert some_condition_here`.\n",
    "\n",
    "We can add additional information to be shown by using a comma. This is often useful when debugging something in the tests, by using `assert False, \"some information here\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmplpn1dw2l.py \u001b[31mF\u001b[0m\u001b[31m                             [100%]\u001b[0m\n",
      "\n",
      "===================== FAILURES =====================\n",
      "\u001b[31m\u001b[1m_______________ test_additional_info _______________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_additional_info\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mWe\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mre on a \u001b[39;49;00m\u001b[33m{\u001b[39;49;00msys.platform\u001b[33m}\u001b[39;49;00m\u001b[33m system\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: We're on a linux system\u001b[0m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-11-60a514e0d01f>\u001b[0m:4: AssertionError\n",
      "============= short test summary info ==============\n",
      "FAILED tmplpn1dw2l.py::test_additional_info - Ass...\n",
      "\u001b[31m================ \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.02s\u001b[0m\u001b[31m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "import sys\n",
    "\n",
    "\n",
    "def test_additional_info():\n",
    "    assert False, f\"We're on a {sys.platform} system\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, pytest takes care of:\n",
    "\n",
    "- Automatically discovering test functions\n",
    "- Running them all\n",
    "- Displaying failures in an useful way\n",
    "\n",
    "## Options\n",
    "\n",
    "pytest takes various command-line options to customize its behavior. The most common ones are:\n",
    "\n",
    "### Output\n",
    "\n",
    "| Option | Behavior |\n",
    "| -- |: -- |\n",
    "| `-v` (`--verbose`) | **Verbose output, i.e. show test names** |\n",
    "| `-s` (`--capture=off`) | **Disable output capturing** |\n",
    "| `--setup-show` | **Show information about fixtures being used** |\n",
    "| `--tb` | Control traceback generation (`auto`, `long`, `short`, `line`, `native`, `no`) |\n",
    "\n",
    "### Information\n",
    "\n",
    "| Option | Behavior |\n",
    "| -- |: -- |\n",
    "| `--markers` | **List all available markers** |\n",
    "| `--fixtures` | **List all available fixtures** |\n",
    "| `--help` | **Show all available options** |\n",
    "\n",
    "### Test selection\n",
    "\n",
    "| Option | Behavior |\n",
    "| -- |: -- |\n",
    "| `-k`, `-m` | **Filter based on name (*k*eyword) or marker** |\n",
    "| `-x` (`--exitfirst`) | Exit instantly on first failure |\n",
    "| `--lf` (`--last-failed`) | Only run tests which failed on last run |\n",
    "| `--ff` (`--failed-first`) | Run last failed tests first, then run the rest |\n",
    "| `--lw` (`--stepwise`) | Run until the first failure, then next time continue from there |\n",
    "\n",
    "We will use the ones marked in bold in this lab, but the rest might be useful once you've written a handful of tests for a project. Run pytest with `--help` to see all available options.\n",
    "\n",
    "You can use those options with the `ipytest` by adding them to the same line, e.g. `%%run_pytest[clean] -v`.\n",
    "\n",
    "## Output capturing\n",
    "\n",
    "By default, pytest will only show output for failing tests. This means we can use `print(...)` in tests to output some additional information. If the test passes, the output from the `print` is not displayed. If the test fails (e.g. by doing `assert False`), the output for that test will be shown:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmpgkmu65yp.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                            [100%]\u001b[0m\n",
      "\n",
      "===================== FAILURES =====================\n",
      "\u001b[31m\u001b[1m___________________ test_failing ___________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing\u001b[39;49;00m():\n",
      "        \u001b[96mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mm a failing test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-12-089d53948b2d>\u001b[0m:6: AssertionError\n",
      "--------------- Captured stdout call ---------------\n",
      "I'm a failing test\n",
      "============= short test summary info ==============\n",
      "FAILED tmpgkmu65yp.py::test_failing - assert False\n",
      "\u001b[31m=========== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.03s\u001b[0m\u001b[31m ============\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "\n",
    "def test_passing():\n",
    "    print(\"I'm a passing test\")\n",
    "\n",
    "\n",
    "def test_failing():\n",
    "    print(\"I'm a failing test\")\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behavior can be overridden using the `-s` (`--capture=off`) option. This will cause output from tests to show immediately. Note that this will mean that output from the tests and pytest will be mixed, consider adding `-v` to make it more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmp081gp5f9.py::test_passing I'm a passing test\n",
      "\u001b[32mPASSED\u001b[0m\n",
      "tmp081gp5f9.py::test_failing I'm a failing test\n",
      "\u001b[31mFAILED\u001b[0m\n",
      "\n",
      "===================== FAILURES =====================\n",
      "\u001b[31m\u001b[1m___________________ test_failing ___________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_failing\u001b[39;49;00m():\n",
      "        \u001b[96mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mI\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mm a failing test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert False\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-13-089d53948b2d>\u001b[0m:6: AssertionError\n",
      "============= short test summary info ==============\n",
      "FAILED tmp081gp5f9.py::test_failing - assert False\n",
      "\u001b[31m=========== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.03s\u001b[0m\u001b[31m ============\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -s -v\n",
    "\n",
    "\n",
    "def test_passing():\n",
    "    print(\"I'm a passing test\")\n",
    "\n",
    "\n",
    "def test_failing():\n",
    "    print(\"I'm a failing test\")\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markers\n",
    "\n",
    "pytest lets us \"mark\" tests to group related tests together, even across different files. Those marks use Python's decorator syntax. You don't need to know how decorators work in detail to use marks, but if you're interested, there's a [Real Python guide](https://realpython.com/primer-on-python-decorators/) on the topic.\n",
    "\n",
    "To mark a test, we need to do `import pytest` and add a `@pytest.mark.`*something* line (a decorator) right over the test function. For example, we could write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_markers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_markers.py\n",
    "\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow():\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "def test_fast():\n",
    "    pass  # do nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to register those markers in a `pytest.ini` config file, to ensure that pytest knows which markers exist and that we didn't accidentally introduce a typo in the `@pytest.mark` decorator. Currently, if we run pytest, we get a warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "[...]\n",
      "\n",
      "test_markers.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 2.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_markers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To register the markers, we create a new `pytest.ini` file listing and documenting the available markers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest.ini\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest.ini\n",
    "[pytest]\n",
    "markers =\n",
    "  slow: Tests which take a while to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can now run pytest normally - here, we'll include `-v` to see the test names being run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "[...]\n",
      "\n",
      "test_markers.py::test_slow \u001b[32mPASSED\u001b[0m\u001b[32m                                        [ 50%]\u001b[0m\n",
      "test_markers.py::test_fast \u001b[32mPASSED\u001b[0m\u001b[32m                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 2.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v test_markers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now want to only run the slow test, we can do so via `-m slow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "[...]\n",
      "collected 2 items / 1 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "test_markers.py::test_slow \u001b[32mPASSED\u001b[0m\u001b[32m                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[32m in 2.02s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v -m slow test_markers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the **1 deselected** in the output above. Similarly, we can pass a Python-like expression to `-m`, using keywords like `and`, `or` and `not`. Note that we'll need to quote the argument to avoid it being split by the shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "[...]\n",
      "collected 2 items / 1 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "test_markers.py::test_fast \u001b[32mPASSED\u001b[0m\u001b[32m                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m1 deselected\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v -m \"not slow\" test_markers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the slow test wasn't being run, and thus the total runtime took almost 0s instead of around 2s.\n",
    "\n",
    "To see all available markers (including pytest's built-in ones), we can run pytest with `--markers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m@pytest.mark.slow:\u001b[0m Tests which take a while to run\n",
      "\n",
      "[...]\n",
      "\n",
      "\u001b[1m@pytest.mark.skip(reason=None):\u001b[0m skip the given test function with an optional reason. Example: skip(reason=\"no way of currently testing this\") skips the test.\n",
      "\n",
      "\u001b[1m@pytest.mark.skipif(condition, ..., *, reason=...):\u001b[0m skip the given test function if any of the conditions evaluate to True. Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. See [`skipif` in the pytest reference docs](https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif) for more information.\n",
      "\n",
      "\u001b[1m@pytest.mark.xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict):\u001b[0m mark the test function as an expected failure if any of the conditions evaluate to True. Optionally specify a reason for better reporting and run=False if you don't even want to execute the test function. If only specific exception(s) are expected, you can list them in raises, and if the test fails in other ways, it will be reported as a true failure. See [`xfail` in the pytest reference docs](https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail) for more information.\n",
      "\n",
      "\u001b[1m@pytest.mark.parametrize(argnames, argvalues):\u001b[0m call a test function multiple times passing in different arguments in turn. argvalues generally needs to be a list of values if argnames specifies only one name or a list of tuples of values if argnames specifies multiple names. Example: @parametrize('arg1', [1,2]) would lead to two calls of the decorated test function, one with arg1=1 and another with arg1=2. See [`parametrize` in the pytest reference docs](https://docs.pytest.org/en/stable/parametrize.html) for more info and examples.\n",
      "\n",
      "[...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pytest --markers test_markers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we want to skip a test in certain conditions - perhaps because a test can't run on a certain operating system. To do so, pytest provides two built-in marks, `skip` and `skipif`. Using the `skip` mark skips a test unconditionally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmplrokre5e.py \u001b[33ms\u001b[0m\u001b[33m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m================ \u001b[33m\u001b[1m1 skipped\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[33m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.skip\n",
    "def test_nothing_interesting():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a skip reason, which is shown when `-v` is given (depending on the terminal width):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmprdsu1ksg.py::test_x \u001b[33mSKIPPED\u001b[0m (Don't fe...)\u001b[33m [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m================ \u001b[33m\u001b[1m1 skipped\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[33m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -v\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.skip(reason=\"Don't feel like running this\")\n",
    "def test_x():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `skipif` instead of `skip` lets us add a condition as the first argument (giving a `reason` is then mandatory) - thus, the above is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmpz_sx1a9m.py::test_x \u001b[33mSKIPPED\u001b[0m (Don't fe...)\u001b[33m [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m================ \u001b[33m\u001b[1m1 skipped\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[33m ================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -v\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.skipif(True, reason=\"Don't feel like running this\")\n",
    "def test_x():\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the `True`, we can use any condition, similar like in an `if`.\n",
    "\n",
    "## Parametrizing\n",
    "\n",
    "Often, the same test should run with different sets of data. If we had a `power` function similar to what you've seen in the functions chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power(basis, exponent):\n",
    "    result = basis ** exponent\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to test several different values to see how the function handles those. To do so, we could write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmp7x5ijhfw.py::test_power_two \u001b[32mPASSED\u001b[0m\u001b[32m        [ 33%]\u001b[0m\n",
      "tmp7x5ijhfw.py::test_power_ten \u001b[32mPASSED\u001b[0m\u001b[32m        [ 66%]\u001b[0m\n",
      "tmp7x5ijhfw.py::test_power_zero_exp \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================ \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -v\n",
    "\n",
    "\n",
    "def test_power_two():\n",
    "    assert power(2, 10) == 1024\n",
    "\n",
    "\n",
    "def test_power_ten():\n",
    "    assert power(10, 3) == 1000\n",
    "\n",
    "\n",
    "def test_power_zero_exp():\n",
    "    assert power(10, 0) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this gets cumbersome, especially when the test is multiple lines which would need to be copy-pasted.\n",
    "\n",
    "Instead, pytest lets us add a special `parametrize` marker (note the spelling, \"parametrize\", not \"paramet**e**rize\" nor \"parametri**s**e\" - all of those are valid English spellings, but pytest only accepts the first).\n",
    "In the decorator, we need to specify:\n",
    "\n",
    "- The names of the arguments we'd like to parametrize, as a single string (*not* multiple strings!)\n",
    "- Their values, as a list of tuples\n",
    "\n",
    "Thus, we could rewrite the above as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmpmmdkmpo2.py::test_power[2-10-1024] \u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0m\n",
      "tmpmmdkmpo2.py::test_power[10-3-1000] \u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0m\n",
      "tmpmmdkmpo2.py::test_power[10-0-1] \u001b[32mPASSED\u001b[0m\u001b[32m    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================ \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -v\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"base, exponent, result\",\n",
    "    [\n",
    "        (2, 10, 1024),\n",
    "        (10, 3, 1000),\n",
    "        (10, 0, 1),\n",
    "    ],\n",
    ")\n",
    "def test_power(base, exponent, result):\n",
    "    assert power(base, exponent) == result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how pytest has automatically generated names for the different test cases and they still run as three separate tests.\n",
    "\n",
    "## Fixtures\n",
    "\n",
    "For more complex tests, often there's some kind of data or preparation needed by different tests. A very central concept in pytest to separate and organize such data or setup/cleanup needed for tests are *fixtures*.\n",
    "\n",
    "To use fixtures, we define a *fixture function*, which returns a value we're later going to use in a test. A fixture function is a normal Python function decorated with `@pytest.fixture`. The function then typically does one of three things:\n",
    "\n",
    "- Do some kind of preparation steps for a test (e.g. load a configuration file)\n",
    "- Return some kind of data or object needed for the tests\n",
    "- Return some kind of utility object useful for writing the tests\n",
    "\n",
    "For example, we could use:\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def answer():\n",
    "    return 42\n",
    "```\n",
    "\n",
    "to define a fixture called `answer`. Tests can now use this fixture by having an argument with the *same name* as the fixture function:\n",
    "\n",
    "```python\n",
    "def test_answer(answer):\n",
    "    assert answer == 42\n",
    "```\n",
    "\n",
    "Note that the `answer` variable inside the test function is the value *returned by* the fixture function - in other words, imagine pytest doing something like `test_answer(answer())` when running your test.\n",
    "\n",
    "If we now run all this, the test passes as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmpm9k83ha_.py \u001b[32m.\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================ \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def answer():\n",
    "    return 42\n",
    "\n",
    "\n",
    "def test_answer(answer):\n",
    "    assert answer == 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can freely combine multiple fixtures to arrange our test setup as we see fit - tests can use multiple fixtures, and fixtures can use other fixtures themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmphciliz6s.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================ \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def institute():\n",
    "    return \"INS\"\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def half():\n",
    "    return 21\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def answer(half):\n",
    "    return half * 2\n",
    "\n",
    "\n",
    "def test_half(half):\n",
    "    assert half == 21\n",
    "\n",
    "\n",
    "def test_answer_and_institute(answer, institute):\n",
    "    assert answer == 42\n",
    "    assert institute == \"INS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run pytest with `--fixtures`, we see all available fixtures, including built-in ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixtures also provide a lot of more advanced features, which we won't get into as part of this course:\n",
    "\n",
    "- Fixtures can do *cleanup*/teardown, i.e. run some code *after* each test using them, for example to terminate an external process or close a connection to a database.\n",
    "- pytest can be instructed to *cache* the fixture function result, so that the fixture only gets called once per test file (or even only once for the entire test session). This can be dangerous as it weakens the isolation between individual tests, but often is needed when a certain setup step takes a long time and would be too expensive to do with every test.\n",
    "- Similarly to how tests can be parametrized, fixtures can be parametrized - for example, if we'd write some kind of tool which can talk to appliances from two different manifactures, we might want to run all tests against both of them and expect them to run in the same way for both.\n",
    "- Fixtures can run implicitly (*autouse*), so that its side-effects (e.g. some kind of preparation) are done for every test, even if the test doesn't take the fixture as an argument.\n",
    "\n",
    "## Built-in fixtures\n",
    "\n",
    "pytest exposes much of its functionality using built-in fixtures. We will look at three of them in more detail: `tmp_path`, `monkeypatch` and `capsys`.\n",
    "\n",
    "### tmp_path\n",
    "\n",
    "With the `tmp_path` fixture, we can get an empty temporary directory for every test. It represents a [pathlib](https://docs.python.org/3/library/pathlib.html) object, details about which you will learn in the next lab. For this lab, you only need to know three things about `pathlib` objects:\n",
    "\n",
    "- You can use the `/` operator to chain paths. If you have a `tmp_path`, doing `file_path = tmp_path / test.txt` will give you a new `pathlib` object representing a `test.txt` file in the `tmp_path` folder.\n",
    "- You can use `.write_text(...)` to write text to a `pathlib` object. Doing `file_path.write_text(\"Hello World\")` will result in a `test.txt` file containing `Hello World`.\n",
    "- You can use `.read_text()` on a `pathlib` object to read the text in a file. Doing `file_path.read_text()` will return the `\"Hello World\"` string we wrote into the file earlier.\n",
    "\n",
    "The `tmp_path` fixture is useful if we want to create a file, which is used in some way by our code under test - for example, an input file for a commandline-tool, or some data generated by our code. To keep tests isolated, pytest creates a new directory for every test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmp2ozuz1jq.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                            [100%]\u001b[0m\n",
      "\n",
      "===================== FAILURES =====================\n",
      "\u001b[31m\u001b[1m_____________________ test_one _____________________\u001b[0m\n",
      "\u001b[1m\u001b[31m<ipython-input-38-8f2a005ba3c4>\u001b[0m:2: in test_one\n",
      "    \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m, \u001b[96mstr\u001b[39;49;00m(tmp_path)\n",
      "\u001b[1m\u001b[31mE   AssertionError: /tmp/pytest-of-jovyan/pytest-0/test_one0\u001b[0m\n",
      "\u001b[1m\u001b[31mE   assert False\u001b[0m\n",
      "\u001b[31m\u001b[1m_____________________ test_two _____________________\u001b[0m\n",
      "\u001b[1m\u001b[31m<ipython-input-38-8f2a005ba3c4>\u001b[0m:5: in test_two\n",
      "    \u001b[94massert\u001b[39;49;00m \u001b[94mFalse\u001b[39;49;00m, \u001b[96mstr\u001b[39;49;00m(tmp_path)\n",
      "\u001b[1m\u001b[31mE   AssertionError: /tmp/pytest-of-jovyan/pytest-0/test_two0\u001b[0m\n",
      "\u001b[1m\u001b[31mE   assert False\u001b[0m\n",
      "============= short test summary info ==============\n",
      "FAILED tmp2ozuz1jq.py::test_one - AssertionError:...\n",
      "FAILED tmp2ozuz1jq.py::test_two - AssertionError:...\n",
      "\u001b[31m================ \u001b[31m\u001b[1m2 failed\u001b[0m\u001b[31m in 0.02s\u001b[0m\u001b[31m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] --tb=short\n",
    "\n",
    "\n",
    "def test_one(tmp_path):\n",
    "    assert False, str(tmp_path)\n",
    "\n",
    "\n",
    "def test_two(tmp_path):\n",
    "    assert False, str(tmp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the two tests got different paths based on their names. If we write some data to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmp9mias3q9.py \u001b[31mF\u001b[0m\u001b[31m                             [100%]\u001b[0m\n",
      "\n",
      "===================== FAILURES =====================\n",
      "\u001b[31m\u001b[1m__________________ test_data_file __________________\u001b[0m\n",
      "\n",
      "tmp_path = PosixPath('/tmp/pytest-of-jovyan/pytest-1/test_data_file0')\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_data_file\u001b[39;49;00m(tmp_path):\n",
      "        file_path = tmp_path / \u001b[33m\"\u001b[39;49;00m\u001b[33mtest.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "        file_path.write_text(\u001b[33m\"\u001b[39;49;00m\u001b[33mHello World\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      ">       \u001b[94massert\u001b[39;49;00m file_path.read_text() == \u001b[33m\"\u001b[39;49;00m\u001b[33mHallo Welt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m  \u001b[90m# this will fail\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 'Hello World' == 'Hallo Welt'\u001b[0m\n",
      "\u001b[1m\u001b[31mE         - Hallo Welt\u001b[0m\n",
      "\u001b[1m\u001b[31mE         + Hello World\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m<ipython-input-39-b3ad378ba581>\u001b[0m:4: AssertionError\n",
      "============= short test summary info ==============\n",
      "FAILED tmp9mias3q9.py::test_data_file - Assertion...\n",
      "\u001b[31m================ \u001b[31m\u001b[1m1 failed\u001b[0m\u001b[31m in 0.02s\u001b[0m\u001b[31m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "def test_data_file(tmp_path):\n",
    "    file_path = tmp_path / \"test.txt\"\n",
    "    file_path.write_text(\"Hello World\")\n",
    "    assert file_path.read_text() == \"Hallo Welt\"  # this will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the output above that the file is stored at `/tmp/pytest-of-jovyan/pytest-1/test_data_file0` and investigate manually, e.g. by printing the file via `cat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World"
     ]
    }
   ],
   "source": [
    "!cat /tmp/pytest-of-jovyan/pytest-1/test_data_file0/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monkeypatch\n",
    "\n",
    "The `monkeypatch` fixture lets us temporarily modify some state for a test. Usually we do this when our code calls some function which gets in our way for automatic testing, and we want to replace it by a fake function.\n",
    "\n",
    "Consider this example using `pyinputplus`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyinputplus as pyip\n",
    "\n",
    "\n",
    "def format_mail_from_header():\n",
    "    email = pyip.inputEmail(prompt=\"Type in your email address: \")\n",
    "    return f\"From: Luigi Vercotti <{email}>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to test it without splitting it into two different functions, the input prompt gets in our way for testing - when `pytest` runs the function, you don't want to type in an email every time!\n",
    "\n",
    "We can solve this by writing a custom function which acts like `pyip.inputEmail`, but returns a fixed address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_input_mail(prompt):\n",
    "    return \"luigi.vercotti@example.org\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now tell pytest to replace the function temporarily while the test is running by using the `monkeypatch` fixture, and calling `monkeypatch.setattr(module, \"function_name\", new_function)`. A completed test would thus look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmpa6l3khzo.py \u001b[32m.\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================ \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "def test_format_mail_from_header(monkeypatch):\n",
    "    monkeypatch.setattr(pyip, \"inputEmail\", fake_input_mail)\n",
    "    assert (\n",
    "        format_mail_from_header() == \"From: Luigi Vercotti <luigi.vercotti@example.org>\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### capsys\n",
    "\n",
    "The last built-in fixture we'll look at is `capsys`. Recall how pytest *captures* the output done from tests, so that it doesn't appear when a test passes. Using `capsys` we can access this captured output, and e.g. test functions which use `print()`. To do so, the fixture provides a `readouterr()` function which returns a pair of the standard output (stdout) and error output (stderr):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m=============== test session starts ================\u001b[0m\n",
      "[...]\n",
      "\n",
      "tmp2p3ajhuu.py \u001b[32m.\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m================ \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "\n",
    "def shout():\n",
    "    print(\"We are the knights who say NI!\")\n",
    "\n",
    "\n",
    "def test_shout(capsys):\n",
    "    shout()\n",
    "    stdout, stderr = capsys.readouterr()\n",
    "    assert stdout == \"We are the knights who say NI!\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Getting Started\n",
    "\n",
    "Before you write your first \"real\" test, make yourself familiar again with how to run tests, both directly in the notebook and via an external file.\n",
    "\n",
    "First, let's run a simple test (which does nothing) from the notebook directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Adjust the cell to run the test, then run it.\n",
    "\n",
    "\n",
    "def test_in_notebook():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write the same test to a file and run it that way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Adjust the cell so that its contents are written to test_first.py, then run the cell.\n",
    "\n",
    "\n",
    "def test_in_file():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Run pytest as external process and pass the filename to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Your First Real Test\n",
    "\n",
    "Below, you'll find a `censor_phone_numbers` function, using the regex from the last lab. Complete the function so that it replaces all phone numbers in the input string `inp`.\n",
    "\n",
    "First, run the cell and then verify that it is working manually by calling the function and looking at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PHONE_NUMBER_REGEX = re.compile(r'\\+\\d{9,15}')\n",
    "\n",
    "def censor_phone_numbers(inp):\n",
    "    # todo: Replace all phone numbers in 'inp' and return the result\n",
    "\n",
    "print(censor_phone_numbers(\"You can reach the INS at +41552221838\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we'd now want to improve the regex so that it also matches other phone number formats, for example containing spaces. Manual testing would get cumbersome quickly. Instead, let's write a first test for what we've just tested manually above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "# todo: Write test for censor_phone_numbers with the same input as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Output Capturing\n",
    "\n",
    "We'll move away from our phone number tests for the next exercises, but we'll get back to them later.\n",
    "\n",
    "In the summary above, we've mentioned how `print(...)` acts differently inside pytest, as pytest won't show the output for failing tests.\n",
    "\n",
    "Write a test which fails, and one which passes. In both tests, use `print(...)` to show some output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def test_passing():\n",
    "    # todo: Print some text\n",
    "\n",
    "def test_failing():\n",
    "    # todo: Print some text\n",
    "    # todo: Fail the test (after printing!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run those tests. What's the difference? Run them again but pass `-sv` (`--capture=off --verbose`) to pytest. What changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Using markers\n",
    "\n",
    "Next, we want to group related tests using markers. For this exercise, we'll write our code into a file, because ipytest has [an issue](https://github.com/chmp/ipytest/issues/39) which prevents it from showing certain pytest warnings.\n",
    "\n",
    "To see how markers work, start with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_markers.py\n",
    "\n",
    "import time\n",
    "\n",
    "# todo: Add slow markers\n",
    "\n",
    "\n",
    "def test_slow_1():\n",
    "    time.sleep(2)  # e.g. some expensive calculation\n",
    "\n",
    "\n",
    "def test_slow_2():\n",
    "    time.sleep(2)  # see above\n",
    "\n",
    "\n",
    "def test_fast_1():\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_fast_2():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `pytest` on the unmodified file and observe the runtime duration shown at the end of the pytest output. Next, change the file (edit the cell above and run it again) to add a `slow` marker on the slow tests. Then, register your marker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Write a file to register markers with pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run pytest again, but this time pass `-m \"not slow\"` as option, to only select the fast tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Exercise 5: Skipping Tests\n",
    "\n",
    "We're now back to running pytest with `%%run_pytest` instead of external files.\n",
    "\n",
    "Imagine a test which can't run on your operating system. For example, on the Linux-based Jupyter Hub system, this test will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import os\n",
    "\n",
    "# todo: Skip this test on Linux\n",
    "\n",
    "\n",
    "def test_win():\n",
    "    assert os.sep == \"\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run the test to make sure it fails. Adjust the test to skip it if it's running on a Linux system (hint: see `test_additional_info` from the summary to get an idea how to do so). Finally, run the test again (try with `-v`) to make sure it's skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Parametrizing\n",
    "\n",
    "Let's go back to our phone number tests. In exercise 2 we wrote a single test for the functionality - but that probably doesn't cover everything we want to test. Some ideas:\n",
    "\n",
    "- Does the phone number get censored if it's in the middle of a sentence?\n",
    "- Does it work the same way if the string only contains the phone number, nothing else?\n",
    "- What about a string which contains two phone numbers?\n",
    "- Our regex checks for phone numbers between 9 and 15 digits. Do both of those extremes work as intended?\n",
    "- If we pass a `+` followed by only 8 digits, or followed by 16 digits, does the number (probably not a phone number!) stay unchanged?\n",
    "- If we have a 11-digit number, but with a letter in between, does that stay unchanged?\n",
    "- If we have a phone number, but with spaces in between, does that stay unchanged?\n",
    "- What happens if there's some text like `>>>+41...<<<`, i.e. some characters directly preceding/following the phone number?\n",
    "\n",
    "Most of those questions you can probably answer by carefully looking at the regex - but tests serve as a helpful documentation about exactly those kinds of edge cases. If we change our regex to improve it, the tests make sure everything still works as expected.\n",
    "\n",
    "Instead of writing many different test functions, we'll use pytest's parametrizing feature.\n",
    "\n",
    "- First, start with your test from exercise 2, which tests a single value.\n",
    "- Modify the test so that it does the exact same thing, but using the parametrize functionality (with the same single value for now, and the associated expected output)\n",
    "- Expand the parametrization to cover all cases outlined above. Did you find something which might be a bug in our regex pattern?\n",
    "\n",
    "Note we're running pytest with `-v` here, so that you can see the individual values being tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean] -v\n",
    "\n",
    "# todo: Copy the test you wrote for exercise 2\n",
    "# todo: Add parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Fixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step, we want to make our `censor_phone_numbers` function more generic: It should take a (compiled) regex pattern as an argument, instead of hardcoding the phone number regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def censor_pattern(pattern, inp):\n",
    "    # todo: Replace all texts matching 'pattern' in 'inp' and return the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now want to write tests for the modified functions, those tests need to pass our pattern, `re.compile(r'\\+\\d{9,15}')`, to the `censor_pattern` function.\n",
    "\n",
    "Right now, we only have a single test function (thanks to parametrization) and could easily use the regex pattern in there directly. However, once our tests and code get more complex, it maks sense to separate setup of objects/data needed for our tests into fixtures.\n",
    "\n",
    "- Take your parametrized test from the previous exercise (or you could write a new one, without parametrization, if you prefer).\n",
    "- Adjust the test function to use a new `pattern` argument (which isn't parametrized)\n",
    "- Write a `pattern` fixture function for pytest which returns the compiled pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def pattern():\n",
    "    return\n",
    "\n",
    "\n",
    "# todo: Add \"pattern\" fixture which returns pattern\n",
    "\n",
    "# todo: Add test for censor_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Using tmp_path\n",
    "\n",
    "We'll now move away from our `censor_*` functions again, looking at a couple of built-in pytest fixtures. First, you'll learn how to use `tmp_path`.\n",
    "\n",
    "- Write a new `data_path` fixture function, which uses the `tmp_path` fixture.\n",
    "  * In that fixture, use `tmp_path` to create a new path object, pointing to a `data.txt` in the same directory\n",
    "  * Then, write the text \"Some data used for tests\" to that file\n",
    "  * Finally, return the path object pointing to `data.txt` from the fixture function\n",
    "- Write a test, which:\n",
    "  * Uses the `data_path` fixture\n",
    "  * Reads the text the fixture has written into the file\n",
    "  * Makes sure that the text is what we expect it to be\n",
    "- Find the file in the file system using `!ls` and `!cat` (feel free to open a separate terminal tab if you find it more convenient). You should find it in `/tmp/pytest-of-jovyan` somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "# todo: data_path fixture\n",
    "\n",
    "# todo: test using data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: find file in the filesystem\n",
    "!ls /tmp/pytest-of-jovyan\n",
    "\n",
    "!ls /tmp/pytest-of-jovyan/pytest-2\n",
    "\n",
    "!ls /tmp/pytest-of-jovyan/pytest-2/test_data_path0\n",
    "\n",
    "!cat /tmp/pytest-of-jovyan/pytest-2/test_data_path0/data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: Using monkeypatch\n",
    "\n",
    "The next fixture we'll look into is `monkeypatch`. We want to test a function using the `pyinputplus.inputMenu` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyinputplus as pyip\n",
    "\n",
    "\n",
    "def user_likes_python():\n",
    "    response = pyip.inputMenu([\"Python\", \"C++\", \"Java\"])\n",
    "    return response == \"Python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to run a test for this function without any patching, pytest ask us for input while running the tests - try it out (and use the stop button to interrupt execution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "\n",
    "def test_user_likes_python():\n",
    "    assert (\n",
    "        user_likes_python()\n",
    "    )  # what should we even test here? The outcome depends on what we'd enter..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we now want to patch the `pyip.inputMenu` function away using `monkeypatch`, and make the test pass. To do so, we need to:\n",
    "\n",
    "- Write a function to replace `pyip.inputMenu`, which always returns `\"Python\"`\n",
    "- Use `monkeypatch` in the test, to replace the `\"inputMenu\"` attribute of the `pyip` module by our own function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def fake_input_menu(...):  # todo: what arguments?\n",
    "    # todo: return value\n",
    "\n",
    "def test_user_likes_python():  # todo: add fixture\n",
    "    # todo: use monkeypatch.setattr\n",
    "    assert user_likes_python()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: Using capsys\n",
    "\n",
    "Finally, we'll combine the `monkeypatch` and `capsys` fixtures to test a function which prints something:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def print_interplanetary_greeting():\n",
    "    planet = random.choice(\n",
    "        [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"]\n",
    "    )\n",
    "    print(f\"Hello to all potential lifeforms on {planet}!\")\n",
    "\n",
    "\n",
    "print_interplanetary_greeting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to test this code, we'll notice three problems:\n",
    "\n",
    "- The output depends on randomness\n",
    "- The output is written via `print`, not returned in a way we could easily test it\n",
    "- Pluto is missing :(\n",
    "\n",
    "To fix those problems:\n",
    "\n",
    "- Write a function to replace `random.choice`, which always returns `\"Pluto\"`\n",
    "- Write a test which does:\n",
    "  * Use `monkeypatch` to replace `random.choice` with that function\n",
    "  * Use `capsys` to access the output the test has printed\n",
    "  * Assert that the output matches what you expect (make sure you include the final newline, `\\n`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "# todo: Write function to replace random.choice\n",
    "\n",
    "def test_print_interplanetary_greeting(...):  # todo: add arguments\n",
    "    # todo: Use monkeypatch to patch the \"choice\" attribute of the random module\n",
    "    print_interplanetary_greeting()\n",
    "    # todo: Use capsys to ensure the correct text has been printed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from there\n",
    "\n",
    "If you want to dig more into pytest, here are some further ideas for exercises (note: no solutions given, but feel free to ask your instructor for a live demo):\n",
    "\n",
    "- Read the pytest documentation on [caching fixture values](https://docs.pytest.org/en/6.2.x/fixture.html#scope-sharing-fixtures-across-classes-modules-packages-or-session). Write a fixture which sleeps a couple of seconds (to simulate generating some data), play with the different `scope` settings and observe the result.\n",
    "- Read the pytest documentation on [using fixtures implicitly via autouse](https://docs.pytest.org/en/6.2.x/fixture.html#autouse-fixtures-fixtures-you-don-t-have-to-request). Write multiple tests which need to use `monkeypatch` in the same way (e.g. to patch `random.choice` away). Move the patching into an `autouse=True` fixture so that it's done automatically for all tests.\n",
    "- Find out how to write [lambda functions](https://realpython.com/python-lambda/) in Python. Revisit the test from exercise 9 above, and use a lambda in the `monkeypatch.setattr` call instead of the named `fake_input_menu` function. Next, parametrize the test with the value \"chosen by the user\" (i.e. returned from our fake function - either `Python`, `Java` or `C++`) as well as the expected result (`True` or `False`) so that our test can test all three possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "After these exercises, you should now be familiar with how to write tests for your code! Feel free to ask questions if you got stuck somewhere, are confused about something, or just curious!\n",
    "\n",
    "We encourage you to write tests for the code you write in the following labs, to get into a habit of testing your code automatically rather than manually. Some of the later labs will also introduce more pytest features we haven't covered here yet (e.g. asserting expected exceptions in tests).\n",
    "\n",
    "While we've looked into a lot of different pytest features in this lab, testing is a very wide topic - we've barely scratched the surface! If you start writing more complex applications, consider looking through the [pytest documentation](https://docs.pytest.org/) for more advanced topics and best practices."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
